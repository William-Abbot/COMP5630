Hello, and welcome to our presentation on learning optimal go play with machine learning. So, for some background, go is an ancient board game that is notoriously difficult for computers to solve. though, as of 2007, it has been solved on a 5x5 board. And that is part of our goal, to train a machine learning algorithm to play perfect moves on a 5x5 board and then see if the model can preform well on 7x7 boards. To do this, we are using a multi-layer perceptron algorithm which will take in 25 inputs, one for each board space, and return 25 outputs. And in order to train this model on a 5x5 boards, we convolve the 7x7 board to fit the 5x5 model. How the multi-layer perceptron works is like this, it contains many regular perceptrons organized into layers, with each previous layer feeding into the next. If you remember, the regular perceptron algorithm is trying to approximate the function y = f(w^T * x +b) in order to map input data x to to labels y, updating the wheights along the way. In the multilayer perceptron though, each layer represents that function, and the weights are updated differently using back propagation, which will be further explained in the next slide. As you can see, for each layer, the weight matrix is updated using the learning rate, a, times the partial derivative of the cost function with respect to any weight or bias. Next, as previously states, the input to this model will be an array of 25 values that represents the board state, with 0 being an empy cell or point, 1 being a white stone, and zero being a black stone.